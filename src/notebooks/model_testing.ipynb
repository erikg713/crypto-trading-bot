# 📦 Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

plt.style.use("seaborn-v0_8-darkgrid")
%matplotlib inline

# 📁 Paths to future data/model
X_TEST_PATH = "../data/test/X_test.csv"
Y_TEST_PATH = "../data/test/y_test.csv"
MODEL_PATH = "../models/model.pkl"

# 📂 Load Test Data
try:
    X_test = pd.read_csv(X_TEST_PATH)
    y_test = pd.read_csv(Y_TEST_PATH).squeeze()
    print(f"✅ Loaded test data: {X_test.shape[0]} samples, {X_test.shape[1]} features")
except FileNotFoundError:
    print("⚠️ Test data not found. Please place X_test.csv and y_test.csv in ../data/test/")
    X_test = None
    y_test = None

# 🔌 Placeholder: Load Trained Model
def load_model(path):
    # Replace this with actual model loading code later
    print("ℹ️ Model loading not implemented yet.")
    return None

# 🔮 Placeholder: Generate Predictions
def predict(model, X):
    # Replace with: return model.predict(X)
    print("ℹ️ Prediction function not implemented yet.")
    return np.random.choice([0, 1], size=len(X))  # Random dummy predictions

# 🧪 Evaluate Model
def evaluate_predictions(y_true, y_pred):
    print("📊 Classification Report:")
    print(classification_report(y_true, y_pred))
    print(f"✅ Accuracy: {accuracy_score(y_true, y_pred):.2%}")

    # Plot confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# 🧪 Run Evaluation (only if data is loaded)
if X_test is not None and y_test is not None:
    model = load_model(MODEL_PATH)
    y_pred = predict(model, X_test)
    evaluate_predictions(y_test, y_pred)


from pathlib import Path

# Define the notebook content as a Python code cell sequence
notebook_cells = [
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": "# 📘 Model Testing Notebook\n\nThis notebook is a placeholder for testing models once you've trained them. It includes:\n- Test data loading\n- Placeholder model loading and prediction\n- Evaluation pipeline (accuracy, confusion matrix)"
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 📦 Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

plt.style.use("seaborn-v0_8-darkgrid")
%matplotlib inline
"""
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 📁 Paths to future data/model
X_TEST_PATH = "../data/test/X_test.csv"
Y_TEST_PATH = "../data/test/y_test.csv"
MODEL_PATH = "../models/model.pkl"
"""
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 📂 Load Test Data
try:
    X_test = pd.read_csv(X_TEST_PATH)
    y_test = pd.read_csv(Y_TEST_PATH).squeeze()
    print(f"✅ Loaded test data: {X_test.shape[0]} samples, {X_test.shape[1]} features")
except FileNotFoundError:
    print("⚠️ Test data not found. Please place X_test.csv and y_test.csv in ../data/test/")
    X_test = None
    y_test = None
"""
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 🔌 Placeholder: Load Trained Model
def load_model(path):
    # Replace this with actual model loading code later
    print("ℹ️ Model loading not implemented yet.")
    return None
"""
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 🔮 Placeholder: Generate Predictions
def predict(model, X):
    # Replace with: return model.predict(X)
    print("ℹ️ Prediction function not implemented yet.")
    return np.random.choice([0, 1], size=len(X))  # Random dummy predictions
"""
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 🧪 Evaluate Model
def evaluate_predictions(y_true, y_pred):
    print("📊 Classification Report:")
    print(classification_report(y_true, y_pred))
    print(f"✅ Accuracy: {accuracy_score(y_true, y_pred):.2%}")

    # Plot confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()
"""
    },
    {
        "cell_type": "code",
        "metadata": {},
        "source": """\
# 🧪 Run Evaluation (only if data is loaded)
if X_test is not None and y_test is not None:
    model = load_model(MODEL_PATH)
    y_pred = predict(model, X_test)
    evaluate_predictions(y_test, y_pred)
"""
    }
]

# Create a full Jupyter notebook JSON structure
notebook_content = {
    "cells": notebook_cells,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

# Save the notebook
notebook_path = Path("/mnt/data/model_testing.ipynb")
import json
with open(notebook_path, "w") as f:
    json.dump(notebook_content, f)

notebook_path.name
